# Melanoma Detection
> In this assignment, you will build a multiclass classification model using a custom convolutional neural network in TensorFlow.

> Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution that can evaluate images and alert dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.

## Problem statement

> To build a multiclass classification CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. 

## Data Information 

> The dataset consists of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC). All images were sorted according to the classification taken with ISIC, and all subsets were divided into the same number of images, with the exception of melanomas and moles, whose images are slightly dominant.

The data set contains the following diseases:

* Actinic keratosis
* Basal cell carcinoma
* Dermatofibroma
* Melanoma
* Nevus
* Pigmented benign keratosis
* Seborrheic keratosis
* Squamous cell carcinoma
* Vascular lesion 

## Project Pipeline

We have build a custom multiclass classification model using the CNN Architecture.

> Data Preparation 

* We need to either keep the dataset in local and try to access or we can mount the dataset from the Google Drive.
* Defining the Train and Test data directories path to be used for further process
* We have visualized 1 instance from each class of the images.
* We need to define the batch size, image height and width (which we have been using here as 180) and input size.
* We have defined the training and validation dataset and implemented Autotuning.

> CNN Model Architecture Terminology and Implementations

* We need to Rescale the input from [0,255] to [0,1].
* Convolutional layers apply a convolution operation to the input, passing the result to the next layer. A convolution converts all the pixels in its receptive field into a single value. For example, if you would apply a convolution to an image, you will be decreasing the image size as well as bringing all the information in the field together into a single pixel.
* Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.
* The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. The Dropout implementation usually results in some loss of data but to reduce the features and keep only relevant one's we require to implement.
* Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer.
* The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer.
* The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.
* The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. The main advantage of using Softmax is the output probabilities range. The range will 0 to 1, and the sum of all the probabilities will be equal to one.

> Implementations 

* We build the CNN model by combining all the layers defined above where we add a Conv2D Layer, MaxPool2D, Dropout, Flatten and Dense Layer and use reLu and Softmax as the Activation functions.
* We have Augmented the data using Augmentor library and treated the Data Imbalance.
* We will compile the model and trained the model for specified epochs. 
* After the above model building and training, we have visualized the accuracy and the loss by plotting them. 

## Conclusions

- We have built the model, compiled and trained it as well and after performing all the above steps, we have avoided the Model Overfitting.
- The model accuracy came around 41% whereas the validation accuracy was around 42%.
- The loss was also reduced in the final model as compared to the initial basic models.
- Though the accuracy is still low, which we can increase by doing a few more tuning and testing various parameters for each model.

## Contact
Created by [@SonamKul9527] - feel free to contact me!